# 宏基因组分析流程详细版本

![image-20241214160445588](C:\Users\黎春莲\AppData\Roaming\Typora\typora-user-images\image-20241214160445588.png)

## **一、 **数据前处理

### 生成处理的list文件

```
ls -d -1 /data/lcl/cohort/ID_NAME/data/*_1*.fastq.gz > forward.list
ls -d -1 /data/lcl/cohort/ID_NAME/data/*_2*.fastq.gz > reverse.list
sed 's@.*/@@' forward.list | sed 's/_1*//' | cut -d _ -f 1 | sed 's/\.fastq\.gz//' > samples.list
sed 's|.*/||' forward.list > samples1.list
sed 's|.*/||' reverse.list > samples2.list
```

## 二、**去除接头**

### 创建文件夹

```
mkdir Trimmed
mkdir Trim.log
```

### 方法1：线程启动

```
cat samples.list | parallel -j 30 --xapply "/home/lcl/BBtools/bbmap/bbduk.sh \
in1=/data/lcl/cohort/ID_NAME/data/{}_1.fastq.gz \
in2=/data/lcl/cohort/ID_NAME/data/{}_2.fastq.gz \
out1=$PWD/Trimmed/{}_1_trimmed.fastq.gz \
out2=$PWD/Trimmed/{}_2_trimmed.fastq.gz \
ref=adapters ktrim=r k=23 mink=11 hdist=1 tpe tbo \
&> $PWD/Trim.log/{}.log"
```

### 方法2：for 循环启动（大型数据处理使用）

```
for sample in $(cat samples.list); do
 /home/lcl/BBtools/bbmap/bbduk.sh \
  in1=/data/lcl/cohort/ID_NAME/data/${sample}_1.fastq.gz \
  in2=/data/lcl/cohort/ID_NAME/data/${sample}_2.fastq.gz \
  out1=$PWD/Trimmed/${sample}_1_trimmed.fastq.gz \
  out2=$PWD/Trimmed/${sample}_2_trimmed.fastq.gz \
  ref=adapters ktrim=r k=23 mink=11 hdist=1 tpe tbo \
  &> $PWD/Trim.log/${sample}.log || true &
 wait
done
```

## 三、**质量控制**

### 创建文件夹

```
mkdir -p Qcontrol/loss
```

```
parallel -j 30 --xapply "/home/lcl/sickle-master/sickle pe -f $PWD/Trimmed/{1}_1_trimmed.fastq.gz -r  $PWD/Trimmed/{1}_2_trimmed.fastq.gz -o $PWD/Qcontrol/{1}_1_trimmed.fastq  -p $PWD/Qcontrol/{1}_2_trimmed.fastq -s $PWD/Qcontrol/loss/{1}_singleton.fastq.gz -t sanger -q 20 -l 100 &> $PWD/Trim.log/{1/.}.log" :::: samples.list
```

### 压缩文档供下步使用

```
cd Qcontrol 
parallel -j 30 gzip ::: *.fastq &
```

## **四、过滤人类DNA**

### 进程过滤

### 创建文件夹

```
mkdir filter
mkidr -p filter/humanDNA
mkdir -p fiter/non-humanDNA
```

### 小型数据集请去掉systemd-run --scope -p MemoryLimit=300G

```
systemd-run --scope -p MemoryLimit=300G parallel -j 2 --xapply "/home/lcl/BBtools/bbmap/bbmap.sh ref=/data/lcl/Gender-specific/hg38.fa in1=$PWD/Qcontrol/{1} in2=$PWD/Qcontrol/{2} outu1=$PWD/filter/non-humanDNA/{1} outu2=$PWD/filter/non-humanDNA/{2} outm1=$PWD/filter/humanDNA/{1} outm2=$PWD/filter/humanDNA/{2} minid=0.95 build=1 > $PWD/Trim.log/{1/.}.log 2>&1" :::: samples1.list :::: samples2.list
```

### 生成质量报告

```
fastqc *.fastq.gz &
multiqc . & 
```

## **五、宏基因组覆盖度和序列多样性估计**

### 创建文件夹

```
mkdir nonpareil
```

```
ls $PWD/filter/non-humanDNA/*.fastq.gz | parallel nonpareil -s {} -T kmer -f fastq -b nonpareil/{/.}
```

### R语言代码

```
install.packages("Nonpareil")  # 根据 CRAN 或 Bioconductor 中的实际情况进行安装
library(Nonpareil)
setwd("/data/lcl/cohort_result/Gender-specific/nonpareil/")
#分图来表示
npo_files <- list.files(pattern = "\\.npo$", full.names = TRUE)
#整图表示
par(mfrow = c(1, 1))
# 列出所有 .npo 文件
npo_files <- list.files(pattern = "\\.npo$", full.names = TRUE)
# 创建颜色向量
colors <- rainbow(length(npo_files))
# 使用 Nonpareil.set 绘制多条曲线在一个坐标轴中
Nonpareil.set(npo_files, plot = TRUE, col = colors)
# 添加图例
legend("topright", legend = basename(npo_files), col = colors, lty = 1, cex = 0.8)
#汇总，查看覆盖度
Nonpareil_npo <- Sys.glob(file.path("/data/lcl/cohort_result/Gender-specific/nonpareil/", "*.npo")) 
l <- Nonpareil.curve.batch(Nonpareil_npo,Nonpareil.legend=F,col = "red",alpha=0.3)
summ_coverage <- data.frame(summary(l))
```

## 六、微生物群落的物种水平组成分析

### 小型数据集可以去掉systemd-run --scope -p MemoryLimit=300G 

### 创建文件夹

```
mkdir metaphlan
```

```
systemd-run --scope -p MemoryLimit=300G parallel -j 2 --xapply "metaphlan $PWD/filter/non-humanDNA/{1}_1_trimmed.fastq.gz,$PWD/filter/non-humanDNA/{1}_2_trimmed.fastq.gz --input_type fastq -o $PWD/metaphlan/{1}.txt --nproc 10 --stat_q 0.1 --bowtie2out $PWD/metaphlan/bowtie2out_{1}.bz2 > $PWD/Trim.log/{1}_metaphlan.log 2>&1" :::: samples.list
```

### 处理txt文件的代码

### 创建分类文件夹

先分类

```
mkdir classify
```

选择获取的物种水平

```
mkdir phylum
```

### 启动程序

```
 python process.py $PWD/ $PWD/classify
 python classify.py 
```

### process.py代码内容

```
import pandas as pd 
import os
import sys  # 导入 sys 模块

def metaphlan_result_extract(fold_path, outdir_path):
    fold_names = os.listdir(fold_path)
    for fold_name in fold_names:
        if fold_name.endswith('.txt'):  # metaphlan文件路径
            pre = fold_name.split('_')[0]
            file_path = os.path.join(fold_path, fold_name)
            file = pd.read_table(file_path, comment='#', names=['taxonomy', 'taxid', 'abundance', 'other_possible_taxon'])
            split = file['taxonomy'].str.split('|')
            file['level'] = split.map(lambda x: len(x))
            
            levels = ['kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species', 'SGB']
            for level_num, group in file.groupby('level'):
                if level_num < len(levels):  # 仅在级别名称在 levels 列表范围内时进行保存
                    outfile_path = os.path.join(outdir_path, f"{pre}_{levels[level_num - 1]}.txt")
                    group.to_csv(outfile_path, sep='\t', index=False)

def main():
    if len(sys.argv) < 3:  # 检查参数数量是否为3个
        print("usage: python process.py <input_dir> <outdir>")
        sys.exit(1)
    
    input_dir = sys.argv[1]
    output_dir = sys.argv[2]
    metaphlan_result_extract(input_dir, output_dir)  # 调用函数

if __name__ == "__main__":
    main()
```

### classify代码内容

```
# -*- coding: utf-8 -*-
import os
import pandas as pd

# 手动输入数据文件夹路径和输出文件夹路径
input_folder = input("input:").strip()
output_folder = input("output:").strip()

# 显示层级选项
print("choice:")
print("1. kingdom")
print("2. kingdom, phylum")
print("3. kingdom, phylum, class")
print("4. kingdom, phylum, class, order")
print("5. kingdom, phylum, class, order, family")
print("6. kingdom, phylum, class, order, family, genus")
print("7. kingdom, phylum, class, order, family, genus, species")
print("8. kingdom, phylum, class, order, family, genus, species, form")

# 让用户选择层级
level_choice = input("number:").strip()

# 根据选择的层级定义层级名称
if level_choice == "1":
    levels = ["kingdom"]
    file_suffix = "_kingdom.txt"
elif level_choice == "2":
    levels = ["kingdom", "phylum"]
    file_suffix = "_phylum.txt"
elif level_choice == "3":
    levels = ["kingdom", "phylum", "class"]
    file_suffix = "_class.txt"
elif level_choice == "4":
    levels = ["kingdom", "phylum", "class", "order"]
    file_suffix = "_order.txt"
elif level_choice == "5":
    levels = ["kingdom", "phylum", "class", "order", "family"]
    file_suffix = "_family.txt"
elif level_choice == "6":
    levels = ["kingdom", "phylum", "class", "order", "family", "genus"]
    file_suffix = "_genus.txt"
elif level_choice == "7":
    levels = ["kingdom", "phylum", "class", "order", "family", "genus", "species"]
    file_suffix = "_species.txt"
elif level_choice == "8":
    levels = ["kingdom", "phylum", "class", "order", "family", "genus", "species", "form"]
    file_suffix = "_form.txt"
else:
    print("exit")
    exit()

# 获取文件夹中所有.txt文件，根据选择的层级
file_list = [f for f in os.listdir(input_folder) if f.endswith(file_suffix)]

# 定义一个函数来处理单个文件
def process_file(file_path, output_folder, levels):
    # 读取文件
    df = pd.read_table(file_path)
    
    # 确保文件中有taxonomy列
    if 'taxonomy' not in df.columns:
        print(f"skip")
        return
    
    # 将 taxonomy 列按 `|` 分割
    split_taxonomy = df['taxonomy'].str.split(r'\|')
    
    # 创建一个字典来存储层级信息
    taxonomy_dict = {level: [] for level in levels}
    
    # 按每个层级提取信息
    for i, level in enumerate(levels):
        taxonomy_dict[level] = split_taxonomy.apply(lambda x: x[i] if len(x) > i else None)
    
    # 清除taxonomy中前缀
    for level in levels:
        taxonomy_dict[level] = taxonomy_dict[level].str.replace(r'^[a-z]+__', '', regex=True)
    
    # 创建结果数据框
    result_df = pd.DataFrame(taxonomy_dict)
    
    # 如果有abundance列，添加到结果数据框
    if 'abundance' in df.columns:
        result_df['abundance'] = df['abundance']
    
    # 设置输出文件路径
    output_file = os.path.join(output_folder, os.path.basename(file_path).replace(file_suffix, "_processed.csv"))
    
    # 输出文件
    result_df.to_csv(output_file, index=False)

# 批量处理文件
for file in file_list:
    file_path = os.path.join(input_folder, file)
    process_file(file_path, output_folder, levels)

print("completed")
```



## 七、**序列组装**

### 创建文件夹

```
mkdir SPAdes
```

### 启动bash文件

```
chmod +x SPAdes.sh
sed -i 's/\r$//' SPAdes.sh
./SPAdes.sh
```

### bash代码

```
#!/bin/bash
# 遍历所有 *_1.fastq.gz 文件
for sample in $PWD/filter/non-humanDNA/*_1_trimmed.fastq.gz
do
    # 获取文件名前缀 (去掉 _1.fastq.gz)
    prefix=$(basename $sample "_1_trimmed.fastq.gz")
    # 输出目录（确保目录名称不冲突，避免与文件名相同）
    output_dir="$PWD/SPAdes/outdir_$prefix"
    # 创建输出目录
    mkdir -p $output_dir
    # 运行MetaSPAdes
    metaspades.py -o $output_dir \
                  -1 $PWD/filter/non-humanDNA/${prefix}_1_trimmed.fastq.gz \
                  -2 $PWD/filter/non-humanDNA/${prefix}_2_trimmed.fastq.gz \
                  -t 48 -m 500 -k 21,33,55,77,99,127
done
```

### 使用QUAST评估组装结果

```
mkdir QUAST
chmod +x QUAST.sh
sed -i 's/\r$//' QUAST.sh
./QUAST.sh
```

### QUAST.sh代码

```
#!/bin/bash
# 定义 samples.list 文件的路径
samples_list="$PWD/samples.list"
# 创建一个用于存储QUAST输出目录的列表
quast_dirs=()
# 读取 samples.list 文件并逐行处理
while IFS= read -r sample_id; do
  # 使用 sample_id 作为前缀
  prefix="$sample_id"
  # 构建完整的 contigs.fasta 文件路径
  contigs_fasta="$PWD/SPAdes/outdir_$prefix/contigs.fasta"
  # 构建QUAST输出目录
  quast_output_dir="$PWD/QUAST/quast_results_$prefix"
  # 检查 contigs.fasta 文件是否存在
  if [ -f "$contigs_fasta" ]; then
    # 如果文件存在，运行 QUAST
    quast.py -o "$quast_output_dir" "$contigs_fasta"
    # 将QUAST输出目录添加到列表中
    quast_dirs+=("$quast_output_dir")
  else
    # 如果文件不存在，打印错误消息
    echo "ERROR! File not found (contigs): $contigs_fasta"
  fi
done < "$samples_list"
```

### 整合QUSTA结果

```
chmod +x quast.sh
sed -i 's/\r$//' quast.sh
./quast.sh
```

### quast.sh代码

```
#!/bin/bash
# 定义 samples.list 文件的路径
samples_list="$PWD/samples.list"
# 定义汇总结果的文件路径
summary_file="$PWD/QUAST/summary_report.txt"
# 初始化汇总报告文件
echo -e "Sample_ID\tN50\tL50\tTotal_Length\tContig_Num\tMax_Length" > "$summary_file"
# 读取 samples.list 文件并逐行处理
while IFS= read -r sample_id; do
  # 使用 sample_id 作为前缀
  prefix="$sample_id"
  # 构建 QUAST 结果目录的路径
  quast_results="$PWD/QUAST/quast_results_$prefix"
  # 检查 report.tsv 文件是否存在
  report_file="$quast_results/report.tsv"
  if [ -f "$report_file" ]; then
    # 提取相关信息并写入汇总文件
    n50=$(awk -F"\t" '{if ($1 == "N50") print $2}' "$report_file")
    l50=$(awk -F"\t" '{if ($1 == "L50") print $2}' "$report_file")
    total_length=$(awk -F"\t" '{if ($1 == "Total length") print $2}' "$report_file")
    contig_num=$(awk -F"\t" '{if ($1 == "Number of contigs") print $2}' "$report_file")
    max_length=$(awk -F"\t" '{if ($1 == "Max contig length") print $2}' "$report_file")
    # 将提取的数据添加到汇总文件
    echo -e "$sample_id\t$n50\t$l50\t$total_length\t$contig_num\t$max_length" >> "$summary_file"
  else
    # 如果 report.tsv 文件不存在，打印错误消息
    echo "ERROR! Report file not found for $sample_id: $report_file"
  fi
done < "$samples_list"
echo "汇总完成！结果已保存至 $summary_file"
```

## **八、微生物基因组的分箱**

### 启动程序

```
chmod +x vamb.sh
ed -i 's/\r$//' vamb.sh
./vamb.sh
```

### vamb.sh程序

```
#!/bin/bash
# 从 samples.list 文件读取SRR编号并并行处理
cat $PWD/samples.list | parallel -j 8 '
    SRR={};
    echo "Processing ${SRR}..."
    # 定义输入输出文件和路径
    CONTIGS="$PWD/SPAdes/outdir_${SRR}/contigs.fasta"
    FASTQ1="/data/lcl/cohort/Ten_pairs_done/data/${SRR}_1.fastq.gz"
    FASTQ2="/data/lcl/cohort/Ten_pairs_done/data/${SRR}_2.fastq.gz"
    BAM_DIR="$PWD/bamfiles/bamfiles_${SRR}/"
    OUT_DIR="$PWD/vamb/${SRR}/"
    # 创建BAM文件夹（如果不存在）
    mkdir -p ${BAM_DIR}
    # BWA索引并进行比对，排序BAM文件，生成索引
    bwa index ${CONTIGS} && \
    bwa mem -t 8 ${CONTIGS} ${FASTQ1} ${FASTQ2} | samtools sort -o ${BAM_DIR}/${SRR}.sorted.bam && \
    samtools index ${BAM_DIR}/${SRR}.sorted.bam
    # 运行VAMB进行binning
    vamb bin default --outdir ${OUT_DIR} --fasta ${CONTIGS} --bamdir ${BAM_DIR} --minfasta 200000
    echo "${SRR} processing completed."
'  # 将闭合引号放在 parallel 命令的结尾
```

## **九、checkm使用**

### 启动程序

```
chmod +x checkm.sh
ed -i 's/\r$//' checkm.sh
./checkm.sh
```

### checkm.sh代码

```
#!/bin/bash
# 读取samples.list文件
sample_file="$PWD/samples.list"
# 设置每批处理的样本数量
batch_size=6
# 读取文件的总行数
total_lines=$(wc -l < "$sample_file")
# 计算需要循环的次数
loops=$((total_lines / batch_size))
if [ $((total_lines % batch_size)) -ne 0 ]; then
  loops=$((loops + 1))
fi
# 循环处理每一批样本
for (( i=0; i<loops; i++ )); do
  # 从samples.list中读取下一批样本
  batch_samples=$(tail -n +$((i*batch_size+1)) "$sample_file" | head -n$batch_size)
  # 为每个样本执行expect脚本
  while IFS= read -r sample; do
    # 调用expect脚本，传递当前样本ID
    expect auto_authenticate.exp "$sample" &
    # 获取expect脚本的PID
    pid=$!
    # 将PID添加到后台进程列表中
    pids+=($pid)
  done <<< "$batch_samples"

  # 等待当前批次的expect脚本执行完成
  for pid in "${pids[@]}"; do
    wait $pid
  done
  # 清空后台进程列表，为下一批次做准备
  pids=()
done
```

### auto_authenticate.exp代码

```
#!/usr/bin/expect -f
# 设置超时时间为无限
set timeout -1
# 从参数中获取sample_file变量
set sample [lindex $argv 0]
set current_pwd [exec pwd]
# 启动 sudo 命令
# 使用 -a 选项来指定输入文件
spawn systemd-run --scope -p MemoryLimit=400G parallel -j 2 -v --xapply "checkm lineage_wf -t 4 -x fna --nt --tab_table -f $current_pwd/checkm/{1}/bins_qa.txt $current_pwd/vamb/{1}/bins $current_pwd/checkm/{1}/bins_qa_result > $current_pwd/logs/checkm_{1}.log 2>&1" ::: $sample
# 选择身份 1
expect "Choose identity to authenticate as (1-2):" { send "1\r" }
# 输入密码
expect "Password:" { send "xj20240601\r" }
# 等待命令完成
expect eof
```

### checkm结果汇总

### 启动程序

```
chmod +x checkm_all.sh
ed -i 's/\r$//' checkm_all.sh
./checkm_all.sh
```

### checkm_all.sh代码

```
#!/bin/bash
# 定义存放 checkm 文件的目录
checkm_dir="$PWD/checkm1"  # 替换为实际路径
output_file="$PWD/combined_bins_qa.txt"
# 初始化输出文件，并添加表头（假设所有文件的表头相同）
first_file=$(find "$checkm_dir" -type f -name "bins_qa.txt" | head -n 1)
if [ -f "$first_file" ]; then
    # 提取表头并添加一个新列 "Source" 写入输出文件
    head -n 1 "$first_file" | awk '{print $0 "\tSource"}' > "$output_file"
else
    echo "No bins_qa.txt files found!"
    exit 1
fi
# 遍历所有 bins_qa.txt 文件，跳过表头并追加内容
find "$checkm_dir" -type f -name "bins_qa.txt" | while read -r file; do
    # 提取文件路径中的 SRR 文件名
    srr_name=$(basename "$(dirname "$file")")
    
    # 跳过表头，追加内容并添加 SRR 文件名作为新列
    tail -n +2 "$file" | awk -v source="$srr_name" '{print $0 "\t" source}' >> "$output_file"
done
echo "整合完成！结果保存到 $output_file"
```

## **十、Prodigal基因预测**

### 启动程序

```
mkdir prodigal
sed -i 's/\r//' prodigal.sh
chmod +x prodigal.sh
./prodigal.sh
```

### prodigal.sh代码内容

```
#!/bin/bash
# 设置输入目录、输出目录和样本列表文件
input_dir="$PWD/SPAdes"
output_dir="$PWD/prodigal"
samples_list="samples.list"
# 创建输出目录（如果不存在）
mkdir -p "$output_dir"
# 读取 samples.list 文件中的样本名
while read sample; do
    # 构建contigs.fasta文件的路径
    contig_file="$input_dir/outdir_$sample/contigs.fasta"
    # 检查文件是否存在
    if [ -f "$contig_file" ]; then
        # 创建一个与样本名对应的子目录
        sample_output_dir="$output_dir/$sample"
        mkdir -p "$sample_output_dir"
        # 设置输出文件路径
        gff_file="$sample_output_dir/${sample}_metagenomic.gff"
        faa_file="$sample_output_dir/${sample}_metagenomic_proteins.faa"
        fna_file="$sample_output_dir/${sample}_metagenomic_nucleotides.fna"
        # 使用 prodigal 进行预测
        prodigal -i "$contig_file" -o "$gff_file" -a "$faa_file" -d "$fna_file" -p meta
        echo "已完成预测：$sample"
    else
        echo "警告：文件不存在 $contig_file"
    fi
done < "$samples_list"
```

## **十一、salmon定量**

### 启动程序

```
mkdir bowtie2_results
sed -i 's/\r//' bowtie2.sh
chmod +x bowtie2.sh
./bowtie2.sh
```

### bowtie2.sh代码内容

```
#!/bin/bash
# 假设 samples.list 文件包含了所有样本的ID，例如：ERR1449743, ERR1449744...
# 循环处理 samples.list 中的每个样本
for sample in $(cat samples.list); do
    echo "Processing sample: $sample"
    # 创建输出目录（如果不存在）
    mkdir -p $PWD/bowtie2_results
    # 生成索引
    bowtie2-build $PWD/prodigal/$sample/${sample}_metagenomic_nucleotides.fna \
                  $PWD/prodigal/$sample/index
    # 使用 bowtie2 比对数据
    bowtie2 -x $PWD/prodigal/$sample/index \
            -1 $PWD/filter/non-humanDNA/${sample}_1_trimmed.fastq.gz \
            -2 $PWD/filter/non-humanDNA/${sample}_2_trimmed.fastq.gz \
            -S $PWD/bowtie2_results/${sample}.sam
    # 使用 samtools 将 .sam 文件转换为 .bam 文件
    samtools view -bS $PWD/bowtie2_results/${sample}.sam \
                  > $PWD/bowtie2_results/${sample}.bam
    # 排序 .bam 文件
    samtools sort $PWD/bowtie2_results/${sample}.bam \
                  -o $PWD/bowtie2_results/${sample}_sorted.bam
    # 索引排序后的 .bam 文件
    samtools index $PWD/bowtie2_results/${sample}_sorted.bam
    # 获取 idxstats 输出（每个基因的比对读数）
    samtools idxstats $PWD/bowtie2_results/${sample}_sorted.bam \
                      > $PWD/bowtie2_results/${sample}_idxstats.txt
    echo "Finished processing sample: $sample"
    wait 
done
```

